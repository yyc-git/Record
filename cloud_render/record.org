* TODO build basic application and render demo
render ldraw model(.ldr file)


** DONE update design

design

doc:
requirement
roadmap



** DONE summary collections



** DONE fix .dat bug

test1.dat
3626texshell.dat

should show the head!!!



update render1 service


update application


update ldr-pc



** DONE install win10


test webgpu-rtx->scene2 !


** DONE multi thread render/multi gpu core render

e.g. split sampleCount to servers, ...


*** DONE think how to parallel

multiple render workers

share the:
device
storage buffer(pixel buffer)
uniform buffer(camera buffer, render command buffer)
topLevelAS(or not?)

rt,render pipeline
gbuffer texture





*** TODO implement

test one render worker


test two render workers



test gbuffer texture



*** DONE think how to parallel2

1.我们需要服务器的显卡（RTX2060及以上的显卡）支持光线追踪
2.我们会在服务器上部署云渲染服务，提供给用户使用 （因此需要外网访问服务器，80或443端口）
3.我们需要服务器安装最新的在win10上运行的nvdia->vulkan-driver驱动（如Windows 451.98版本）（驱动下载地址：https://developer.nvidia.com/vulkan-driver -> Vulkan Beta Driver Downloads）
4.我们希望进行GPU调度，如这台服务器若有4个RTX GPU，那么就可以让我们通过代码来调度，从而实现同时让4个或更多的用户同时使用这台服务器上的不同的GPU。
通过代码（SDK）来进行下面的操作，从而让我们可以开发一个自动化调度GPU的工具：
获得有哪些GPU空闲；
设置多少个用户可以同时使用云渲染服务（互相独立不影响）；
设置某个用户使用多大的GPU资源；

5.我们希望联合多个GPU(RTX2060及以上的显卡)形成支持光线追踪的渲染农场，从而让我们可以分布式地并行渲染一个大型3D场景(使用光线追踪)

主要目的：
进行分布式渲染，得到一张3D场景的图片

思路：
其中一台服务器节点（该节点可以为一个CPU或一般的服务器）作为主节点，进行渲染任务的调配;
其他服务器节点（该节点可以为一个GPU或者GPU服务器）作为worker节点，负责渲染该图片的某个方块区域；
他们都共享场景数据；


所有服务器共享场景数据（一个buffer数据）；
主服务器需要传输渲染指令（如渲染该图片的哪个方块区域，使用哪个相机的数据等）；
GPU服务器节点负责把渲染结果（该图片的某个方块区域的数据）传输给主服务器节点；


worker节点需要win10操作系统，因为渲染引擎需要运行在GPU服务器节点上，需要win10系统+vulkan驱动




1台主机，多个GPU





我研究了下，目前渲染引擎没有找到在一台服务器上使用多个GPU的方法。

那么：
1、你这边是否可以把一台服务器的多个GPU聚合为一个GPU，让我像一个GPU那样调用？
如一台服务器有两个显存为11G的RTX 2070s，那么我可以把它当成一个GPU那样来使用（显存为22G）


回答：不能


2、你这边是否可以给我多台服务器（包含一台主服务器和多台worker服务器），每台服务器都是win10系统+只有一个RTX2070s以上的GPU，每台服务器通过http通信（最好用内网通信，速度要快！）？
这样我就可以由主服务器负责通过http通信，将渲染任务分派给各个worker服务器（只有一个GPU），然后将最终结果通过http通信传输给外界用户


回答：
可以。
主服务器和worker服务器的通信速度、主服务器与外部用户的通信速度 要看你的设计要求。



主服务器和worker服务器的通信、主服务器与外部用户的通信 都使用http通信把？

回答：
这个没问题




6.我们后期会上线云游戏服务，因此需要提供良好的网络传输和网络压缩工具等。


** DONE learn and use newest rt features
test VK_KHR_ray_tracing feature! 

https://news.developer.nvidia.com/whats-new-in-nvidia-vkray/

*** DONE learn Callable Shader
https://nvpro-samples.github.io/vk_raytracing_tutorial_KHR/vkrt_tuto_callable.md.html


*** DONE learn Ray Query
https://nvpro-samples.github.io/vk_raytracing_tutorial_KHR/vkrt_tuto_rayquery.md.htm



*** TODO webgpu node should support them!!!



** DONE learn .res

https://reasonml.org/docs/reason-compiler/latest/new-bucklescript-syntax


https://rescript-lang.org/docs/manual/latest/let-binding


*** DONE prepare dev env


////*** TODO update blog!


** DONE think engine's architecture

*** DONE recall DDD


*** DONE recall engine serials



*** DONE design engine

give architecture views

give divide domains view



externals:
WebGPU(so can work with webgpu node and webgpu in browser!!!)(now only inject with webgpu node in the entry!)
File IO




infrastructure:
interface
wonder-re-structure



rename external to dependency;

Wonder.js should be .res project!(not commit lib/, dist/ files)

remove code climate



shader files use .vert, ... instead of json;
job return Stream;
no loop, use run instead;
////jobs has no own data! get/set data to common data region in PO(in Run Domain->PO);
(e.g. get/set pass->pipelines to WebGPU->objects->pipelines, taa pass->isFirstFrame to 数据结构->通用数据->bool data map;

run->po->job data has each job's data!(e.g. gbuffer job's data)



no context postfix(e.g. rename ContextContextEntity to ContextEntity)(if duplicate, add context post fix! e.g. rename ContextEntity to ContextContextEntity);
use Root instead of Entity for Aggregate Root;
not define Root's type(type t);


////split transform to transform + link ?





build domain:
move Repo to dependency;
treat config to be dependency;

remove infrastructure layer;

convert do to po in do model;






** DONE learn spir-v unit test


** DONE learn https://www.gamasutra.com/blogs/NiklasGray/20190611/344437/Syncing_a_dataoriented_ECS_with_a_stateful_external_system.php




////** TODO write 《3D引擎实战精粹》系列

开发方法

架构

并行

优化

工程化




** DONE develop v2.0.0-alpha.1: work in Wonder.js Project

unit test

render test
(local test, not test in .travis!!!)
(change run test in nodejs to render test)

no perf test!

add .cz, test coverage, action(instead of travis), code climate, build status ...



features:
add job manager, gameObject + transform,link component, empty pass jobs(just render all pixels to blue), unit test, render test


then @wang can do:
geometry component;
gbuffer pass;


*** TODO implement

rescript
jest

//.cz


//add gameObject + transform component

//add job manager, init transform+update transform(need optimize) job:

set pipeline dp
register job

set default one



//set all dp


//refactor:
////rename gameObject->addXXX to setXXX
rename transform->addGameObject to setGameObject

//remove RepoAt




//finish init pipelines jobs

//finish run pipelines jobs






//unit test:
    in Construct:


    in CP:
        add test funcs:
        let setInitPipelineData = (data: PipelineCPVOType.pipelineData): unit => {};
        let setRestartPipelineData = (data: PipelineCPVOType.pipelineData): unit => {};
        let setRunPipelineData = (data: PipelineCPVOType.pipelineData): unit => {};




////render test



prepare for publish:

//test coverage
(> 95%)

action(instead of travis)

build status, other bars...

add 1.1 version link

publish



@wang




** TODO develop v2.0.0-alpha.2


*** DONE add all components


**** DONE logic

add pbrMaterial(no shader)

share pbrMaterial




add geometry(custom, sphere geometry)

share geometry






add cameraView, cameraProjection



add directionLight



**** DONE test



*** TODO work in Wonder-Cloud-Picture project! 

**** DONE add ray tracin pass jobs




**** DONE logic


receive the render command for distributed Ray Tracing:
////input: tile region's width,height + .wd
input: .wd(render whole picture)
////output: the tile region data(uint8Array)
output: the whole picture's data(uint8Array)



use high-performance in requestAdapter:
[Exposed=(Window, DedicatedWorker)]
interface GPU {
    Promise<GPUAdapter?> requestAdapter(optional GPURequestAdapterOptions options = {});
};

dictionary GPURequestAdapterOptions {
    GPUPowerPreference powerPreference;
};
enum GPUPowerPreference {
    "low-power",
    "high-performance"
};



add no material shaders



add init pipeline


add restart pipeline




run pipeline add passes:
update transform,
(should build transform update VO for perf!)
////gbuffer,
rt,
generate picture data,
clear






//pass compile;






***** DONE publish;





**** DONE test

***** DONE unit test

jobs:
webgpu

pass tracing

accumulation


***** DONE refactor: cloud-render should use construct's run api instead of do service!

***** DONE refactor: cloud-render should only use construct's run api instead of DPContainer

***** DONE refactor: cloud-render->CPRepoDp shouldn't dependent on CPRepo



***** DONE add Wonder-Example project

use webgpu node to implement webgpu,raytracing interface;



use file api to implement load shader file:
write interface:IFile(refer to chrome native file system:https://web.dev/native-file-system/);
implement IFile with node -> fs;




demo:
(render spheres + planes)










**** DONE publish




** TODO support textures 


*** DONE write demo

////descriptor index

////virtual texture






onion texture demo:
update texture in layers in different frames

scale to max size(2048 x 2048)



not support repeat/clamp-to-edge/mirror-repeat
(should check texCoord in [0.0, 1.0])


*** DONE learn normal map

*** DONE implement

//network dp;

//storage image uint8Array to hash map(key:image id);

//test


//pbr material add textures(diffuseMap, metalRoughnessMap, normalMap, emissionMap)
(setXXXMap(material, imageId))

//test

//geometry add a_texCoord data
(check when getTexCoords)


//test


////add init geometry job
(compute a_tangent data if has normap map)

//add these data to buffer

//test






////add init pbrMaterial texture job
(create texture view, sampler)









//add update textureArray job;

////add TextureArray do service(in webgpu->core->service)

//(only one TextureArray)


//edit init, update path tracing job;
(edit glsl)

//edit render path tracing job;

//test



*** DONE refactor

change all repo dp to return option instead of Js.Nullable.t


*** DONE run test
update example;

fix example:
main loop should only render!


add new scenes:
floor
boomBox



*** DONE add gamma

color,map should all pow(1.0/2.2)!

compare asset->texture(plane)

** DONE webgpu

** DONE publish





////* TODO fix firefly



** TODO learn usdz, support it instead of wd?

references:

https://developer.nvidia.com/usd



https://graphics.pixar.com/usd/docs/index.html
http://graphics.pixar.com/usd/docs/api/index.html
https://github.com/PixarAnimationStudios/USD


youtube:
https://www.youtube.com/watch?v=x9ikzGQW0ys&t=5s



three.js->issue:
https://github.com/mrdoob/three.js/issues/14219



ios:
https://developer.apple.com/cn/augmented-reality/quick-look/


converter:
https://github.com/google/usd_from_gltf
https://github.com/TimvanScherpenzeel/gltf-to-usdz-research

https://developer.apple.com/augmented-reality/tools/ -> Download USDZ tools
(already get tool from wechat->Lin, search "usdpython" in macbook!)



work with nvidia-omniverse-platform;
https://developer.nvidia.com/nvidia-omniverse-platform





** TODO add wd, gltf 

add wd:
wd type,
wd parser


render test





add gltf to wd converter


render test




give demo(render gltf model)






*** TODO refactor: CP Example should only invoke CP API instead of Run API of Wonder.js!




** TODO add blit render job(@wy)


** TODO create Webgl1Renderer project(@wy)

add render_webgl1  pipeline



** TODO develop v2.0.0-alpha.6

*** TODO add others


**** TODO add blit pass

accu pass should after ray tracing pass


**** TODO add taa pass


**** TODO add post effect pass

tonemap
gamma


*** TODO @23 , test render gltf model!

** TODO generate picture data

*** TODO add generate pipeline to generate picture data!

init(only once);
update, render, render, ..., generate;
update, render, render, ..., generate;
...


user can specify:
picture size
sampleCount
camera data(near,far,fovy,aspect; position, target, up)
direction light data(direction, intensity)




add jobs

unit test


render test




*** TODO render test:









** TODO dispose


*** TODO add dispose jobs

should reset geometry buffer;


*** TODO run test: test memory



*** TODO unit test


** TODO perf

compute time:
init,update,render pipeline's time;
the time in [update, render, generate] pipelines;


compute memory



reuse commandBuffer in render raytracing job?

use less geometry container(e.g. the same sphere should in one geometry container)




** TODO add transparent

BSDF

refer to  blender:
https://github.com/KhronosGroup/glTF-Blender-IO/issues/123
disney
...




** TODO publish





** TODO develop v2.0.0-alpha.3


*** TODO add denoise

try to use NVIDIA's Optix Denoiser:
https://github.com/maierfelix/nvk-optix-denoiser
https://research.nvidia.com/publication/interactive-reconstruction-monte-carlo-image-sequences-using-recurrent-denoising
https://developer.nvidia.com/optix-denoiser



more denoiser:
// https://www.ece.ucsb.edu/~psen/PaperPages/removing_MC_noise.html
// https://studios.disneyresearch.com/wp-content/uploads/2019/03/Denoising-Deep-Monte-Carlo-Renderings-1.pdf
https://perso.telecom-paristech.fr/boubek/papers/BCD/
(should use compute shader!)
https://groups.csail.mit.edu/graphics/rendernet/




////**** TODO try fix indirect specular noise




**** TODO compare with bmfr(compare with WebGPU-RTX project)








** TODO move library out to be new projects

*** TODO move Array, Option, Result, ... to Wonder-Re-Structure

TODO refactor:
rename ArraySt, ListSt to Array, List
(because has namespace(WonderReStructure), so not conflict with Array, List!)





"bsc-flags": ["-open WonderReStructure"],


the api is like wonder-commonlib, but the implement can use Belt!



** TODO refactor: update wonder-bs-most,wonder-bs-sinon

stream first!




////** TODO rewrite ldr loader

use wrap type(refer to DDD)

add unit test

add test coverage, travis


** TODO ldr converter

extract from ldr-render1-service


use wrap type(refer to DDD)

add unit test

add test coverage, travis




only generate scene graph data(.wd) for render3:
input: .ldr
output: .wd


////** TODO render3 demo
load scene graph data(.wd);
use render demo to render ldr model file;

(remove denoise, taa pass, use accumulate pass to reduce noise)


*** TODO render test: render ldr model!



** TODO build engine as renderer

render test

unit test

no perf test!

add test coverage, travis, build status ...


receive the render command for distributed Ray Tracing:
input: tile region's width,height + .wd
output: the tile region data(uint8Array)






single render thread
webgpu only
ray tracing only
static scene
data: scene graph data(.wd)


fixed camera(no arcball camera)




use high-performance in requestAdapter:
[Exposed=(Window, DedicatedWorker)]
interface GPU {
    Promise<GPUAdapter?> requestAdapter(optional GPURequestAdapterOptions options = {});
};

dictionary GPURequestAdapterOptions {
    GPUPowerPreference powerPreference;
};
enum GPUPowerPreference {
    "low-power",
    "high-performance"
};







////convert .ldr to .wd;
////(extract service)

load scene graph data(.wd);
use render demo to render ldr model file;


run:
for debug


export:
snapshot data(uint8Array?)

should support stream!????




build dev env, test env;
build ci/cd tool;






optimize:
use less geometry container(e.g. the same sphere should in one geometry container)



(remove denoise, taa pass, use accumulate pass to reduce noise)










** TODO support rounded corner





////** TODO add env map



** TODO publish Wonder 2.0.0-beta

use docker for webgpu env install(e.g. vulkan driver)

** TODO add application

////set camera

////set lights

////add env map


only send ldr file
(store part file in render3 engine cloud)


*** TODO add Main Server


*** TODO add Worker Server


*** TODO add Front End Tool(very basic)


*** TODO add Scheduler

dispatch commands;
sync;
(invoke engine's init, update)



*** TODO add Transporter

compress

decompress



https://stackoverflow.com/questions/36091022/how-compress-typedarrays-arraybuffers-for-storage-and-transmission

https://nodejs.org/api/zlib.html#zlib_zlib
http://www.zlib.net/






** TODO deploy to cloud(distributed render)

我们希望联合多个GPU(RTX2060及以上的显卡)形成支持光线追踪的渲染农场，从而让我们可以分布式地并行渲染一个大型3D场景(使用光线追踪)

主要目的：
进行分布式渲染，得到一张3D场景的图片

思路：
其中一台服务器节点（该节点可以为一个CPU或一般的服务器）作为主节点，进行渲染任务的调配;
其他服务器节点（该节点可以为一个GPU或者GPU服务器）作为worker节点，负责渲染该图片的某个方块区域；
他们都共享场景数据；


所有服务器共享场景数据（一个buffer数据）；
主服务器需要传输渲染指令（如渲染该图片的哪个方块区域，使用哪个相机的数据等）；
GPU服务器节点负责把渲染结果（该图片的某个方块区域的数据）传输给主服务器节点；


worker节点需要win10操作系统，因为渲染引擎需要运行在GPU服务器节点上，需要win10系统+vulkan驱动



solution:
1.use one compute with multi-gpus

https://github.com/gpuweb/gpuweb/issues/995

now not support it!!! will support in pre-1.0(after MVP)


https://github.com/maierfelix/webgpu/issues/26



2.use multiple computes(each one has one gpu)


1、你这边是否可以把一台服务器的多个GPU聚合为一个GPU，让我像一个GPU那样调用？
如一台服务器有两个显存为11G的RTX 2070s，那么我可以把它当成一个GPU那样来使用（显存为22G）


回答：不能


2、你这边是否可以给我多台服务器（包含一台主服务器和多台worker服务器），每台服务器都是win10系统+只有一个RTX2070s以上的GPU，每台服务器通过http通信（最好用内网通信，速度要快！）？
这样我就可以由主服务器负责通过http通信，将渲染任务分派给各个worker服务器（只有一个GPU），然后将最终结果通过http通信传输给外界用户


回答：
可以。
主服务器和worker服务器的通信速度、主服务器与外部用户的通信速度 要看你的设计要求。



主服务器和worker服务器的通信、主服务器与外部用户的通信 都使用http通信把？

回答：
这个没问题






最终方案：
租用一台gpu服务器（一个服务器1个RTX 2070s显卡）作为worker服务器（gpu服务器店铺：https://shop392735765.taobao.com/?spm=2013.1.1000126.2.66cf1e6bdi61c0 );
然后租用一台云服务器作为主服务器，两者通过http传输二进制文件;
主服务器与Front End Tool通过http通信；

一台worker node渲染一整张图片！


可考虑把我这台电脑作为worker服务器！（@王影）





** TODO add weixin login


** TODO add weixin payment


** TODO add website, document


** TODO publish

publish Wonder.js v2.0.0